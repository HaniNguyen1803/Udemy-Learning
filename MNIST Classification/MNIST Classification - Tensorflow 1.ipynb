{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "The dataset provides 28x28 images of handwritten digits (1 per image). With 784 images in total, the goal is to write an algorithm that detects which digit is written. Since there are only 10 digits, this is a classification problem with 10 classes.\n",
    "\n",
    "This code uses Python 3 environment and Tensorflow version 1.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 100\n",
    "\n",
    "# Reset any variables left in memory from previous runs.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Declare placeholders where the data will be fed into.\n",
    "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.float32, [None, output_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST HIDDEN LAYER\n",
    "\n",
    "# Weights and biases for the first linear combination between the inputs and the first hidden layer.\n",
    "# tf.get_variable(\"name\",shape): declare variables, default is Xavier (Glorot)\n",
    "weights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
    "biases_1 = tf.get_variable(\"biases_1\", [hidden_layer_size])\n",
    "\n",
    "# Operation between the inputs and the first hidden layer.\n",
    "# apply relu activations to the linear combinations between inputs and weights, plus the bias.\n",
    "# tf.nn: a module that contains neural network (nn) support. Contains the most commonly used actiovation functions.\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND HIDDEN LAYER\n",
    "\n",
    "# Weights and biases for the second linear combination.\n",
    "# This is between the first and second hidden layers.\n",
    "weights_2 = tf.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.get_variable(\"biases_2\", [hidden_layer_size])\n",
    "\n",
    "# Operation between the first and the second hidden layers.\n",
    "outputs_2 = tf.nn.relu(tf.matmul(outputs_1, weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT LAYER\n",
    "\n",
    "# Weights and biases for the final linear combination.\n",
    "# That's between the second hidden layer and the output layer.\n",
    "weights_3 = tf.get_variable(\"weights_3\", [hidden_layer_size, output_size])\n",
    "biases_3 = tf.get_variable(\"biases_3\", [output_size])\n",
    "\n",
    "# Operation between the second hidden layer and the final output.\n",
    "outputs = tf.matmul(outputs_2, weights_3) + biases_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTION\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\n",
    "\n",
    "# Get the average loss\n",
    "mean_loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZATION\n",
    "\n",
    "# use Adam optimizer\n",
    "# 0.001 is a suboptimal value of the learning rate\n",
    "optimize = tf.train.AdamOptimizer(learning_rate=0.002).minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT\n",
    "\n",
    "# tf.argmax(targets,axis): return the index of the column where logits is highest\n",
    "# tf.equal(): boolean (check is two values are equal)\n",
    "out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "\n",
    "# Get the average accuracy of the outputs.\n",
    "# since out_equals_target is boolean, change it to float for better calculation\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE SESSION\n",
    "\n",
    "# Declare the session variable.\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Initialize the variables. Default initializer is Xavier.\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCHING\n",
    "\n",
    "# batch size = 1: SGD - stochastic gradient descent\n",
    "# batch size = number of samples: GD - gradient descent\n",
    "batch_size = 100\n",
    "\n",
    "# Calculate the number of batches per epoch for the training set.\n",
    "batches_number = mnist.train._num_examples // batch_size\n",
    "\n",
    "# number of epochs.\n",
    "max_epochs = 15\n",
    "\n",
    "# Set the validation loss at high number to not trigger early stopping at the first epoch\n",
    "prev_validation_loss = 9999999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Mean loss: 0.275. Validation loss: 0.121. Validation accuracy: 96.40%\n",
      "Epoch 2. Mean loss: 0.112. Validation loss: 0.099. Validation accuracy: 97.16%\n",
      "Epoch 3. Mean loss: 0.077. Validation loss: 0.091. Validation accuracy: 97.36%\n",
      "Epoch 4. Mean loss: 0.062. Validation loss: 0.089. Validation accuracy: 97.24%\n",
      "Epoch 5. Mean loss: 0.049. Validation loss: 0.087. Validation accuracy: 97.52%\n",
      "Epoch 6. Mean loss: 0.042. Validation loss: 0.086. Validation accuracy: 97.48%\n",
      "Epoch 7. Mean loss: 0.034. Validation loss: 0.088. Validation accuracy: 97.54%\n",
      "End of training.\n"
     ]
    }
   ],
   "source": [
    "# Create a loop for the epochs. This loop will:\n",
    "# 1. Loads 100 inputs and 100 targets (batch_size = 100)\n",
    "# 2. Optimizes the algorithm and calculates the batch loss\n",
    "# 3. Records the loss for the iteration\n",
    "# 4. Starts over with the next 100 batches\n",
    "# 5. Stops when the training set is exhausted\n",
    "\n",
    "for epoch_counter in range(max_epochs):\n",
    "    \n",
    "    # Keep track of the sum of batch losses in the epoch.\n",
    "    curr_epoch_loss = 0.\n",
    "    \n",
    "    # Iterate over the batches in this epoch.\n",
    "    for batch_counter in range(batches_number):\n",
    "        \n",
    "        # Input batch and target batch are assigned values from the train dataset, given a batch size\n",
    "        # mnist.train.next_batch(batch_size): loads the batches one after another\n",
    "        input_batch, target_batch = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # Run the optimization step and get the mean loss for this batch.\n",
    "        # Feed it with the inputs and the targets we just got from the train dataset\n",
    "        _, batch_loss = sess.run([optimize, mean_loss], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "        # Increment the sum of batch losses.\n",
    "        curr_epoch_loss += batch_loss\n",
    "    \n",
    "    # curr_epoch_loss contained the sum of all batches inside the epoch\n",
    "    # Average batch losses over the whole epoch:\n",
    "    curr_epoch_loss /= batches_number\n",
    "    \n",
    "    # At the end of each epoch, get the validation loss and accuracy\n",
    "    # Get the input batch and the target batch from the validation dataset\n",
    "    input_batch, target_batch = mnist.validation.next_batch(mnist.validation._num_examples)\n",
    "    \n",
    "    # Run without the optimization step (simply forward propagate)\n",
    "    validation_loss, validation_accuracy = sess.run([mean_loss, accuracy], \n",
    "        feed_dict={inputs: input_batch, targets: target_batch})\n",
    "    \n",
    "    # Print statistics for the current epoch\n",
    "    # Epoch counter + 1, because epoch_counter automatically starts from 0, instead of 1\n",
    "    print('Epoch '+str(epoch_counter+1)+\n",
    "          '. Mean loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "          '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
    "          '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
    "    \n",
    "    # Trigger early stopping if validation loss begins increasing.\n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "        \n",
    "    # Store this epoch's validation loss to be used as previous validation loss in the next iteration.\n",
    "    prev_validation_loss = validation_loss\n",
    "\n",
    "print('End of training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.61%\n"
     ]
    }
   ],
   "source": [
    "input_batch, target_batch = mnist.test.next_batch(mnist.test._num_examples)\n",
    "test_accuracy = sess.run([accuracy], \n",
    "    feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "test_accuracy_percent = test_accuracy[0] * 100.\n",
    "\n",
    "# Print the test accuracy formatted in percentages\n",
    "print('Test accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
